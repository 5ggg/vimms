{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment: Varying N in top-N DDA fragmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we calculate performance for section 2.3 in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('C:\\\\Users\\\\joewa\\\\Work\\\\git\\\\vimms')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "import numpy as np\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vimms.PlotsForPaper import *\n",
    "from vimms.Common import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = 'C:\\\\Users\\\\joewa\\\\University of Glasgow\\\\Vinny Davies - CLDS Metabolomics Project\\\\'\n",
    "manuscript_data_dir = 'C:\\\\Users\\\\joewa\\\\Work\\\\data\\\\evaluation'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 1: both full-scan and Top-N data are available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load XCMS peak picking results on the ground truth fullscan data. Peak picking was done using the script `extract_peaks.R` in the folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_ms1_intensity = 0\n",
    "rt_range = [(3*60, 21*60)]\n",
    "mz_range = [(0, math.inf)]\n",
    "results_dir = os.path.join(manuscript_data_dir, 'ground_truth\\\\mzML')   \n",
    "csv_file = os.path.join(results_dir, 'extracted_peaks_ms1.csv')\n",
    "P_peaks_df = get_df(csv_file, min_ms1_intensity, rt_range, mz_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count how many ground truth peaks were found by xcms for each fullscan mzML\n",
    "- P = peaks picked by XCMS from the ms1 data (ground truth)\n",
    "- Q = peaks picked by XCMS from the fragmentation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_count_df = P_peaks_df.groupby('filename').size().reset_index(name='counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_count_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to load previous evaluation results, if it exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_file = os.path.join(base_dir, 'Manuscript\\\\2.3. Varying N in Top-N Simulations\\\\result_df.p')\n",
    "try:\n",
    "    result_df = load_obj(df_file)\n",
    "    print(result_df.head())\n",
    "except FileNotFoundError:\n",
    "    result_df = None\n",
    "except AttributeError:\n",
    "    result_df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If not found, then compute a new performance evaluation and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping between experiment folder name to the actual full scan mzML filename in ms1_df\n",
    "experiment_to_filename = {\n",
    "    'beer1pos': 'Beer_multibeers_1_fullscan1.mzML',\n",
    "    'beer2pos': 'Beer_multibeers_2_fullscan1.mzML',\n",
    "    'urine02pos': 'Urine_StrokeDrugs_02_fullscan.mzML',\n",
    "    'urine03pos': 'Urine_StrokeDrugs_03_fullscan.mzML',\n",
    "    'beerqcb': 'QCB_22May19_1.mzML'\n",
    "}\n",
    "experiment_names = list(experiment_to_filename.keys())\n",
    "\n",
    "Ns = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100]\n",
    "rt_tols = [15]\n",
    "\n",
    "matching_mz_tol = 10 # ppm\n",
    "matching_rt_tol = 30 # seconds\n",
    "min_ms1_intensity = 1.75E5 # should be the same as what's used to run the experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if result_df is None:\n",
    "    results = []\n",
    "    for experiment_name in experiment_names: \n",
    "        experiment_out_dir = os.path.join(manuscript_data_dir, experiment_name, 'mzML')\n",
    "        fullscan_filename = experiment_to_filename[experiment_name]\n",
    "        \n",
    "        if experiment_name == 'beerqcb':\n",
    "            min_ms1 = 0\n",
    "        else:\n",
    "            min_ms1 = min_ms1_intensity\n",
    "            \n",
    "        for N in Ns:\n",
    "            for rt_tol in rt_tols:\n",
    "                controller = load_controller(experiment_out_dir, experiment_name, N, rt_tol)\n",
    "                if controller is not None:\n",
    "                    # compute performance for scenario 1\n",
    "                    chemicals = load_obj(os.path.join(experiment_out_dir, 'dataset.p'))           \n",
    "                    tp, fp, fn, prec, rec, f1 = compute_performance_scenario_1(controller, chemicals, min_ms1,\n",
    "                                                                               fullscan_filename, P_peaks_df,\n",
    "                                                                               matching_mz_tol, matching_rt_tol)      \n",
    "                    \n",
    "                    print('%s N=%d rt_tol=%d tp=%d fp=%d fn=%d prec=%.3f rec=%.3f f1=%.3f' % (experiment_name, \n",
    "                        N, rt_tol, tp, fp, fn, prec, rec, f1))\n",
    "                    res = (experiment_name, N, rt_tol, tp, fp, fn, prec, rec, f1)    \n",
    "                    results.append(res)  \n",
    "\n",
    "    result_df = pd.DataFrame(results, columns=['experiment', 'N', 'rt_tol', 'TP', 'FP', 'FN', 'Prec', 'Rec', 'F1'])\n",
    "    save_obj(result_df, df_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a column to the dataframe for the group (beer or urine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_group_column(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df['N'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exclude beer_qcb from the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = result_df['experiment'].isin(['beerqcb'])\n",
    "filtered_df = result_df[~mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_df = result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 30})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "ax = sns.lineplot(x='N', y='Prec', hue='experiment', legend='brief', data=filtered_df)\n",
    "plt.legend(prop={'size': 14})\n",
    "# plt.title('Precision (Alternative Case)')\n",
    "for l in ax.lines:\n",
    "    plt.setp(l, linewidth=5)\n",
    "plt.ylabel('Precision')\n",
    "plt.xlabel(r'Top-$N$')\n",
    "plt.tight_layout()\n",
    "plt.savefig('topN_precision.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "ax = sns.lineplot(x='N', y='Rec', hue='experiment', legend='brief', data=filtered_df)\n",
    "plt.legend(prop={'size': 14})\n",
    "# plt.title('Recall (Alternative Case)')\n",
    "for l in ax.lines:\n",
    "    plt.setp(l, linewidth=5)\n",
    "plt.ylabel('Recall')\n",
    "plt.xlabel(r'Top-$N$')\n",
    "plt.tight_layout()\n",
    "plt.savefig('topN_recall.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "ax = sns.lineplot(x='N', y='F1', hue='experiment', legend='brief', data=filtered_df)\n",
    "plt.legend(prop={'size': 14})\n",
    "# plt.title('Fragmentation Performance (Alternative Case)')\n",
    "for l in ax.lines:\n",
    "    plt.setp(l, linewidth=5)\n",
    "plt.ylabel(r'$F_{1}\\;score$')\n",
    "plt.xlabel(r'Top-$N$')\n",
    "plt.tight_layout()\n",
    "plt.savefig('topN_f1.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot XCMS peak picking results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to evaluate how the quality of peak picking reduces as we increase N.\n",
    "\n",
    "- P = peaks picked by XCMS from the ms1 data (ground truth)\n",
    "- Q = peaks picked by XCMS from the fragmentation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Does Q decrease as N increases?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load peak picking results for all mzMLs generated by the simulator in all experiments (beer1pos, beer2pos, urine02pos, urine03pos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = os.path.join(manuscript_data_dir, 'ground_truth\\\\mzML')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_ms1_intensity = 0\n",
    "rt_range = [(3*60, 21*60)]\n",
    "mz_range = [(0, math.inf)]\n",
    "experiment_names = ['beer1pos', 'beer2pos', 'urine02pos', 'urine03pos', 'beerqcb']\n",
    "experiment_names = ['beer1pos', 'beer2pos', 'urine02pos', 'urine03pos']\n",
    "dfs = []\n",
    "for experiment_name in experiment_names:\n",
    "    print('Loading %s' % experiment_name)\n",
    "    csv_file = os.path.join(manuscript_data_dir, '%s\\\\mzML\\\\extracted_peaks_ms1.csv' % experiment_name)\n",
    "    df = get_df(csv_file, min_ms1_intensity, rt_range, mz_range)\n",
    "    dfs.append(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine all the peak picking results and count how many peaks for each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_peaks = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_peaks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_df = Q_peaks.groupby('filename').size().reset_index(name='counts')\n",
    "count_df['N'] = count_df.apply (lambda row: get_N(row), axis=1)\n",
    "count_df[['N']] = count_df[['N']].astype('int')\n",
    "count_df['rt_tol'] = count_df.apply (lambda row: get_dew(row), axis=1)\n",
    "count_df[['rt_tol']] = count_df[['rt_tol']].astype('int')\n",
    "count_df['experiment_name'] = count_df.apply(lambda row: row['filename'].split('_')[1], axis=1)\n",
    "count_df['group'] = count_df.apply(lambda row: experiment_group(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual_count = count_df[count_df['filename'].str.contains('multibeers') | count_df['filename'].str.contains('StrokeDrugs')]\n",
    "# experimental_count = count_df[~count_df['filename'].str.contains('multibeers') & ~count_df['filename'].str.contains('StrokeDrugs')]\n",
    "# experimental_count = experimental_count.sort_values(by=['experiment_name', 'N'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_rt_tol = 15\n",
    "Q_count = count_df[count_df['rt_tol'] == selected_rt_tol]\n",
    "Q_count = Q_count.sort_values(by=['experiment_name', 'N', 'rt_tol'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_count.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = Q_count['experiment_name'].isin(['beerqcb'])\n",
    "filtered_Q_count = Q_count[~mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax = sns.lineplot(x='N', y='counts', data=filtered_Q_count, hue='experiment_name', markers=True)\n",
    "# plt.title('Number of MS1 features in fragmentation files')\n",
    "for l in ax.lines:\n",
    "    plt.setp(l, linewidth=5)\n",
    "plt.legend(prop={'size': 20})\n",
    "plt.ylabel('Counts')\n",
    "plt.xlabel(r'Top-$N$')\n",
    "plt.tight_layout()\n",
    "plt.savefig('topN_num_peaks.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What proportion of Q peaks are in P peaks (and are not)? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matches(row):\n",
    "    # get the fullscan and fragmentation mzML file names for the current row\n",
    "    Q_filename = row['filename']\n",
    "    Q_group = Q_filename.split('_')[1]\n",
    "    P_filename = experiment_to_filename[Q_group]\n",
    "\n",
    "    # extract peaks picked by XCMS from that file and turn them into Chemicals\n",
    "    Q_chemicals = df_to_chemicals(Q_peaks, Q_filename)  \n",
    "    P_chemicals = df_to_chemicals(P_peaks_df, P_filename)\n",
    "    \n",
    "    mz_tol = 10\n",
    "    rt_tol = 10\n",
    "    matches = match(P_chemicals, Q_chemicals, mz_tol, rt_tol, verbose=False)\n",
    "    prop = len(matches) / len(P_chemicals)\n",
    "    print('%s matches = %d/%d (%f)' % (Q_filename, len(matches), len(P_chemicals), prop))    \n",
    "    return prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Q_count['matches'] = Q_count.apply(lambda row: get_matches(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mask = Q_count['experiment_name'].isin(['beerqcb'])\n",
    "filtered_Q_count = Q_count[~mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax = sns.lineplot(x='N', y='matches', data=filtered_Q_count, hue='experiment_name', markers=True)\n",
    "# plt.title('Proportion of MS1 features in fragmentation files that are also found in full-scan files', size=20)\n",
    "for l in ax.lines:\n",
    "    plt.setp(l, linewidth=5)\n",
    "plt.legend(prop={'size': 24})  \n",
    "plt.ylabel('Matches')\n",
    "plt.xlabel(r'Top-$N$')\n",
    "plt.tight_layout()\n",
    "plt.savefig('topN_prop_matched.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 2:  only Top-N DDA is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_file_2 = os.path.join(base_dir, 'Manuscript\\\\2.3. Varying N in Top-N Simulations\\\\result_df_2.p')\n",
    "try:\n",
    "    result_df_2 = load_obj(df_file_2)\n",
    "    print(result_df_2.head())\n",
    "except FileNotFoundError:\n",
    "    result_df_2 = None\n",
    "except AttributeError:\n",
    "    result_df_2 = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If not found, then compute a new performance evaluation and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if result_df_2 is None:\n",
    "    results = []\n",
    "    for experiment_name in experiment_names:         \n",
    "        experiment_out_dir = os.path.join(manuscript_data_dir, experiment_name, 'mzML')\n",
    "        fullscan_filename = experiment_to_filename[experiment_name]                \n",
    "        \n",
    "        if experiment_name == 'beerqcb':\n",
    "            min_ms1 = 0\n",
    "        else:\n",
    "            min_ms1 = min_ms1_intensity\n",
    "        \n",
    "        for N in Ns:\n",
    "            for rt_tol in rt_tols:\n",
    "                \n",
    "                # load chemicals and check for matching\n",
    "                chemicals = load_obj(os.path.join(experiment_out_dir, 'dataset.p'))           \n",
    "                fragfile_filename = 'experiment_%s_N_%d_rttol_%d.mzML' % (experiment_name, N, rt_tol) \n",
    "\n",
    "                # load controller and compute performance\n",
    "                controller = load_controller(experiment_out_dir, experiment_name, N, rt_tol)\n",
    "                if controller is not None:\n",
    "                    tp, fp, fn, prec, rec, f1 = compute_performance_scenario_2(controller, chemicals, min_ms1,\n",
    "                                                                               fullscan_filename, fragfile_filename,\n",
    "                                                                               P_peaks_df, Q_peaks, matching_mz_tol, matching_rt_tol)\n",
    "                    print('%s N=%d rt_tol=%d tp=%d fp=%d fn=%d prec=%.3f rec=%.3f f1=%.3f' % (experiment_name, \n",
    "                        N, rt_tol, tp, fp, fn, prec, rec, f1))\n",
    "                    res = (experiment_name, N, rt_tol, tp, fp, fn, prec, rec, f1)    \n",
    "                    results.append(res)  \n",
    "\n",
    "    result_df_2 = pd.DataFrame(results, columns=['experiment', 'N', 'rt_tol', 'TP', 'FP', 'FN', 'Prec', 'Rec', 'F1'])\n",
    "    save_obj(result_df_2, df_file_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a column to the dataframe for the group (beer or urine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_group_column(result_df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exclude beer_qcb from plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = result_df_2['experiment'].isin(['beerqcb'])\n",
    "filtered_result_df_2 = result_df_2[~mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_result_df_2 = filtered_result_df_2[filtered_result_df_2['rt_tol'] == 15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_result_df_2['N'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_result_df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "ax = sns.lineplot(x='N', y='Prec', hue='experiment', legend='brief', data=filtered_result_df_2)\n",
    "# plt.title('Precision')\n",
    "for l in ax.lines:\n",
    "    plt.setp(l, linewidth=5)\n",
    "plt.ylabel('Precision')\n",
    "plt.xlabel(r'Top-$N$')\n",
    "plt.legend(prop={'size': 20})\n",
    "plt.tight_layout()\n",
    "plt.savefig('topN_precision_2.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "ax = sns.lineplot(x='N', y='Rec', hue='experiment', legend='brief', data=filtered_result_df_2)\n",
    "# plt.title('Recall')\n",
    "for l in ax.lines:\n",
    "    plt.setp(l, linewidth=5)\n",
    "plt.ylabel('Recall')\n",
    "plt.xlabel(r'Top-$N$')\n",
    "plt.legend(prop={'size': 20})\n",
    "plt.tight_layout()\n",
    "plt.savefig('topN_recall_2.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "ax = sns.lineplot(x='N', y='F1', hue='experiment', legend='brief', data=filtered_result_df_2)\n",
    "# plt.title('Fragmentation Performance (F1-score)', fontsize=24)\n",
    "for l in ax.lines:\n",
    "    plt.setp(l, linewidth=5)\n",
    "plt.legend(prop={'size': 20})\n",
    "plt.xlabel(r'Top-$N$')\n",
    "plt.ylabel(r'$F_{1}\\;score$')\n",
    "plt.tight_layout()\n",
    "plt.savefig('topN_f1_2.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(12, 6))\n",
    "# ax = sns.lineplot(x='N', y='TP', hue='experiment', legend='brief', data=filtered_result_df_2)\n",
    "# # plt.title('TP')\n",
    "# for l in ax.lines:\n",
    "#     plt.setp(l, linewidth=5)\n",
    "# plt.legend(prop={'size': 20})\n",
    "# plt.tight_layout()\n",
    "# plt.savefig('topN_tp_2.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(12, 6))\n",
    "# ax = sns.lineplot(x='N', y='FP', hue='experiment', legend='brief', data=filtered_result_df_2)\n",
    "# # plt.title('FP')\n",
    "# for l in ax.lines:\n",
    "#     plt.setp(l, linewidth=5)\n",
    "# plt.tight_layout()\n",
    "# plt.savefig('topN_fp_2.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(12, 6))\n",
    "# ax = sns.lineplot(x='N', y='FN', hue='experiment', legend='brief', data=filtered_result_df_2)\n",
    "# # plt.title('FN')\n",
    "# for l in ax.lines:\n",
    "#     plt.setp(l, linewidth=5)\n",
    "# plt.tight_layout()\n",
    "# plt.savefig('topN_fn_2.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make fancy 3D plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment_name = 'beerqcb'\n",
    "# experiment_out_dir = os.path.join(manuscript_data_dir, experiment_name, 'mzML')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_data_file = os.path.join(base_dir, 'Manuscript\\\\2.3. Comparison of Multiple Settings within Top N Simulations\\\\plot_data.p')\n",
    "# try:\n",
    "#     plot_data = load_obj(plot_data_file)\n",
    "# except FileNotFoundError:\n",
    "#     plot_data = None\n",
    "# except AttributeError:\n",
    "#     plot_data = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if plot_data is None:    \n",
    "#     Ns = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100]\n",
    "#     rt_tols = [1, 5, 10, 15, 20, 25, 30, 45, 60, 90, 120]    \n",
    "    \n",
    "#     X, Y = np.meshgrid(Ns, rt_tols)\n",
    "#     Z_precision = np.zeros_like(X).astype(float)\n",
    "#     Z_recall = np.zeros_like(X).astype(float)\n",
    "#     Z_f1 = np.zeros_like(X).astype(float)\n",
    "    \n",
    "#     for j in range(X.shape[1]):\n",
    "#         for i in range(X.shape[0]):    \n",
    "#             N = X[i, j]\n",
    "#             rt_tol = Y[i, j]            \n",
    "#             analysis_name = 'experiment_N_%d_rttol_%d' % (N, rt_tol) \n",
    "            \n",
    "#             # load the list of chemicals that we put into the simulator for each experiment        \n",
    "#             experiment_out_dir = os.path.join(manuscript_data_dir, experiment_name, 'mzML')\n",
    "#             dataset = load_obj(os.path.join(experiment_out_dir, 'dataset.p'))           \n",
    "#             fullscan_filename = experiment_to_filename[experiment_name]            \n",
    "\n",
    "#             # load controller and compute performance\n",
    "#             fragfile_filename = 'experiment_beer1pos_N_%d_rttol_%d.mzML' % (N, rt_tol)             \n",
    "#             controller = load_controller(experiment_out_dir, experiment_name, N, rt_tol)\n",
    "#             if controller is not None:\n",
    "#                 tp, fp, fn, prec, rec, f1 = compute_performance_scenario_2(controller, chemicals, min_ms1_intensity,\n",
    "#                                                                            fullscan_filename, fragfile_filename,\n",
    "#                                                                            P_peaks_df, Q_peaks, matching_mz_tol, matching_rt_tol)\n",
    "                \n",
    "#                 print('%s N=%d rt_tol=%d tp=%d fp=%d fn=%d prec=%.3f rec=%.3f f1=%.3f' % (experiment_name, \n",
    "#                     N, rt_tol, tp, fp, fn, prec, rec, f1))\n",
    "#                 Z_precision[i, j] = prec\n",
    "#                 Z_recall[i, j] = rec\n",
    "#                 Z_f1[i, j] = f1\n",
    "                \n",
    "#     plot_data = {\n",
    "#         'X': X,\n",
    "#         'Y': Y,\n",
    "#         'Z_precision': Z_precision,\n",
    "#         'Z_recall': Z_recall,\n",
    "#         'Z_f1': Z_f1\n",
    "#     }\n",
    "#     save_obj(plot_data, plot_data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_plot(X, Y, Z, xlabel, ylabel, zlabel, title, out_file=None):\n",
    "#     # Plot the surface.\n",
    "#     fig = plt.figure()\n",
    "#     ax = fig.gca(projection='3d')\n",
    "#     surf = ax.plot_surface(X, Y, Z, cmap=cm.coolwarm,\n",
    "#                            linewidth=0, antialiased=False)\n",
    "\n",
    "#     # Customize the z axis.\n",
    "#     # ax.set_zlim(-1.01, 1.01)\n",
    "#     # ax.zaxis.set_major_locator(LinearLocator(10))\n",
    "#     # ax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\n",
    "\n",
    "#     # Add a color bar which maps values to colors.\n",
    "#     fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "#     ax.set_xlabel(xlabel)\n",
    "#     ax.set_ylabel(ylabel)\n",
    "#     ax.set_zlabel(zlabel)    \n",
    "#     plt.title(title)\n",
    "#     plt.tight_layout()\n",
    "#     if out_file is not None:\n",
    "#         plt.savefig(out_file, dpi=300)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = plot_data['X']\n",
    "# Y = plot_data['Y']\n",
    "# Z_precision = plot_data['Z_precision']\n",
    "# Z_recall = plot_data['Z_recall']\n",
    "# Z_f1 = plot_data['Z_f1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_plot(X, Y, Z_precision, \n",
    "#           'N', 'Dynamic exclusion window (s)', 'Precision', 'Precision with varying Ns and dynamic exclusion windows',\n",
    "#          out_file='plot_3d_precision.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_plot(X, Y, Z_recall, \n",
    "#           'N', 'Dynamic exclusion window (s)', 'Recall', 'Recall with varying Ns and dynamic exclusion windows',\n",
    "#            out_file='plot_3d_recall.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_plot(X, Y, Z_f1, \n",
    "#           'N', 'Dynamic exclusion window (s)', 'F_1', 'F_1 score with varying Ns and dynamic exclusion windows',\n",
    "#          out_file='plot_3d_f1.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
